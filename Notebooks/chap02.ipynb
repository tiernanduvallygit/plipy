{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Crash Course in Parsing and Lexing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{parse tree}\n",
    "\\index{derivation}\n",
    "-->\n",
    "In this chapter we get our first glimpse of actual programming language implementation.\n",
    "A good place to start with is the implementation of syntax analysis.\n",
    "Every language processor in our language processor classification performs some sort of syntax analysis (with the exception of the generator).\n",
    "As we saw in the previous chapter, the structure of a programming language can be captured by rule sets called grammars.\n",
    "Therefore, we begin this chapter with a more detailed discussion of grammars including derivations and parse trees, and we\n",
    "will take a look at how grammars are used to specify the syntax of programming languages.  We will use a grammar to \n",
    "specify our first programming language called Exp0.\n",
    " \n",
    " <!--\n",
    "\\index{parser}\n",
    "\\index{top-down parsing}\n",
    "\\index{bottom-up parsing}\n",
    "\\index{LL(k) parsing}\n",
    "\\index{LR(k) parsing}\n",
    "-->\n",
    "Grammars are extremely useful for specifying the structure of programming languages, but what makes them even more powerful as a\n",
    "language specification tool is that we can easily convert grammars to programs that can recognize the structure of a programming\n",
    "language, *i.e.*, we can convert grammars to programs that perform symbol grouping in an input stream according\n",
    "to the syntax rules of the language.  Programs that recognize the structure of a programming language are called parsers.\n",
    "The algorithms that parsers use for grouping symbols in an input stream come in two flavors: (a) top-down or LL(k) parsing and\n",
    "(b) bottom-up or LR(k) parsing.  The discussion of grammars cannot be complete without talking about lexical analysis.\n",
    "You can think of lexical analysis as a preprocessing step for parsing that break the symbol stream into discernable\n",
    "tokens that can be processed by the grammar.\n",
    "\n",
    "<!--\n",
    "\\index{parser generator}\n",
    "\\index{PLY}\n",
    "-->\n",
    "Programs that convert grammars into parsers are called parser generators.  Here and for the remainder of the book we will be using\n",
    "a parser generator called PLY.  PLY is a bottom-up parser generator that reads a grammar specification and produces\n",
    "a parser written in Python.  PLY actually contains both a parser generator and a lexical analyzer genration tool. We conclude the chapter with the construction of a couple of  language processors.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammars\n",
    "\n",
    "## The Basics\n",
    "\n",
    "<!--\n",
    "\\index{grammar}\n",
    "\\index{syntactic structure}\n",
    "\\index{grammar rule}\n",
    "\\index{context-free grammar}\n",
    "\\index{context-sensitive grammar}\n",
    "\\index{regular grammar}\n",
    "-->\n",
    "Grammars are sets of rules that define the syntactic structure of a programming language as we have seen in the rule\n",
    "set in the last chapter.\n",
    "Rules in grammars specify how symbols, words, and phrases are combined to make up valid sentences, *i.e.* programs.\n",
    "When we talk about grammars in the context of specifying programming languages we usually mean context-free grammars as opposed to\n",
    "context-sensitive or regular grammars, for example.\n",
    "In context-free grammars the rules have a very specific form: there is only a single symbol on the left side of a rule and there are\n",
    "zero or more symbols on the right side of a rule.  The fact that we allow a rule to have no symbols on the right means that a rule\n",
    "allows us to derive *nothing* or in more meaningful syntactical terms, that a rule with no symbols on the right side \n",
    "allows us to derive the empty string. \n",
    "We often write the empty string explicitly rather than writing a rule with no symbols on the right side for easier readability. \n",
    "<!--\n",
    "%For the remainder of the book we use the terms grammar and context-free grammar interchangeably with the understanding that we \n",
    "%always mean context-free grammars.\n",
    "-->\n",
    "\n",
    "<!--\n",
    "\\index{arithmetic expression}\n",
    "-->\n",
    "Consider the following  grammar that specifies the syntactic structure of arithmetic expressions with variables  `x`,\n",
    "`y`, and `z`:\n",
    "```\n",
    "Sentence : Expression\n",
    "Expression : Expression + Expression\n",
    "           | Expression - Expression\n",
    "           | Expression * Expression\n",
    "           | Expression / Expression\n",
    "           | ( Expression )\n",
    "           | x\n",
    "           | y\n",
    "           | z\n",
    "```\n",
    "The grammar states that a sentence in this language is an expression and expressions can be add, subtract, multiply, or divide expressions\n",
    "as well as parenthesized expressions and variable names.\n",
    "Note that the parentheses are part of the language that we are defining, that is, the parentheses are part of the syntax of the language.\n",
    "Also note that the '`|`' operator denotes alternatives in production as in: an expression can be a addition expression\n",
    "or a subtraction expression *etc*.\n",
    "\n",
    "<!--\n",
    "\\index{derivation}\n",
    "\\index{valid sentence}\n",
    "-->\n",
    "A grammar allows us to derive any valid sentence in the language that it defines by applying the rules to symbols appearing\n",
    "in the derivation until no further application is possible.\n",
    "In our case,\n",
    "we can derive the sentence `x + y` from the symbol `Sentence` in the above grammar as follows,\n",
    "```\n",
    "Sentence =>\n",
    "Expression =>\n",
    "Expression + Expression =>\n",
    "x + Expression =>\n",
    "x + y\n",
    "```\n",
    "That means, the sentence `x + y` is a valid sentence in this language.\n",
    "\n",
    "<!--\n",
    "\\index{grammar production}\n",
    "\\index{non-terminal (symbol)}\n",
    "\\index{terminal (symbol)}\n",
    "\\index{start symbol}\n",
    "-->\n",
    "We associate specific terminology with grammars.\n",
    "The rules are called *productions* and the symbols appearing on the left sides of productions are called \n",
    "*non-terminals.*\n",
    "The non-terminal set for the grammar above is,\n",
    " ```\n",
    " { Sentence, Expression }.\n",
    " ```\n",
    "Any symbol appearing in the grammar productions that is not part of the non-terminal set,\n",
    "*i.e.*, any symbol that does not appear on the\n",
    "left side of a production, is called a *terminal*.  The terminal set of our grammar is,\n",
    "```\n",
    "{ +, -, *, /, (, ), x, y, z }.\n",
    "```\n",
    "The terminals make up the syntax of a programming language.\n",
    "Note that the parentheses are part of the terminal set, that is, they are part of the language that we are defining.\n",
    "\n",
    "Each grammar has one non-terminal that is always used to start all the derivations in that grammar. That non-terminal is called the *start symbol*.\n",
    "In our grammar the symbol `Sentence` is the start symbol.  It is a convention that the start symbol is the non-terminal defined by the\n",
    "first rule in the grammar, that is, the start symbol is the symbol on the left side of the first rule of the grammar.  We can summarize grammar terminology as follows,\n",
    "\n",
    ">A grammar consists of the following:\n",
    "* A set of productions which are rules with a single symbol on the left and zero or more symbols on the right.\n",
    "* A symbol on the left side of a production is called a non-terminal.\n",
    "* Any symbol in the grammar that is not a non-terminal is called a terminal.\n",
    "* We have a special non-terminal symbol called the start-symbol.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivations\n",
    "\n",
    "<!--\n",
    "\\index{derivation}\n",
    "\\index{valid sentence}\n",
    "-->\n",
    "Given our grammar terminology, we can now be precise of what we mean by a derivation and valid sentence,\n",
    "\n",
    ">A derivation is a sequence of steps that begins with the start symbol and at \n",
    "each derivation step replaces a single non-terminal \n",
    "with the right side of a production that has that non-terminal on the left side.\n",
    "A valid sentence in the language of a grammar is a sequence of symbols that contains only terminals and is arrived at through a derivation\n",
    ".\n",
    "\n",
    "<!--\n",
    "\\index{left-most derivation}\n",
    "\\index{right-most derivation}\n",
    "\\index{parsing algorithm}\n",
    "-->\n",
    "Let us use the grammar defined above that specifies simple arithmetic expressions and show that the sentence \n",
    "```\n",
    "x * (y + z)\n",
    "```\n",
    "is a valid sentence in the language of that grammar.\n",
    "In order to show that it is a valid sentence we have to show that we can derive it from the start symbol `Sentence`:\n",
    "```\n",
    "Sentence =>\n",
    "Expression =>\n",
    "Expression * Expression =>\n",
    "x * Expression =>\n",
    "x * ( Expression ) =>\n",
    "x * ( Expression + Expression ) =>\n",
    "x * ( y + Expression ) =>\n",
    "x * ( y + z )\n",
    "```\n",
    "Here the last derivation step shows that `x * (y + z)` is a valid sentence.\n",
    "If you look closely at the derivation then you will discover that we \n",
    "have consistently expanded the left-most non-terminal at each step of the derivation.  \n",
    "This kind of derivation is called the *left-most derivation*.\n",
    "Consistently expanding the right-most non-terminal at each step of a derivation is called the *right-most derivation*,\n",
    "```\n",
    "Sentence =>\n",
    "Expression =>\n",
    "Expression * Expression =>\n",
    "Expression * ( Expression ) =>\n",
    "Expression * ( Expression + Expression ) =>\n",
    "Expression * ( Expression + z ) =>\n",
    "Expression * ( y + z ) =>\n",
    "x * ( y + z )\n",
    "```\n",
    "If a sentence is valid then it does not matter which derivation we chose to derive it from the start symbol, both derivation techniques will\n",
    "derive the same sentence as long as we make the same choices in terms of which rules to apply.\n",
    "Later on we will see that we classify parsing algorithms according to the derivation they construct for a program.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Trees\n",
    "\n",
    "---\n",
    "<center>\n",
    "<img src=\"figures/chap02/1/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 1. A parse tree for the expression `x + y * z`.\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{parse tree}\n",
    "\\index{derivation}\n",
    "-->\n",
    "\n",
    "Derivations can be represented as tree structures. \n",
    "Consider the left-most derivation of the sentence,\n",
    "```\n",
    "x + y * z\n",
    "```\n",
    "using our grammar for arithmetic expressions from above we can construct the following derivation for\n",
    "this sentence,\n",
    "```\n",
    "Sentence =>\n",
    "Expression =>\n",
    "Expression + Expression =>\n",
    "x + Expression =>\n",
    "x + Expression * Expression =>\n",
    "x + y * Expression =>\n",
    "x + y * z \n",
    "```\n",
    "We construct a parse tree for this derivation by making the start symbol `Sentence` the root node of the tree.\n",
    "Then for every non-terminal that we replace\n",
    "in the derivation we write the right side of the rule that we use to replace the non-terminal underneath it in\n",
    "the tree and connect all the symbols that we just\n",
    "placed underneath the non-terminal to the non-terminal symbol itself.\n",
    "Figure 1 shows the parse tree constructed in this way for the derivation above.\n",
    "The leaves of the parse tree spell out the sentence that we wanted to derive and we highlighted this by placing the derived sentence into\n",
    "a box at the bottom of the figure.\n",
    "We can now say the following,\n",
    "\n",
    "> A parse tree is a tree based representation of a derivation in a grammar.\n",
    "\n",
    "The following movie is an animation of the parse tree construction for the expression `x + y * z`.\n",
    "\n",
    "<!-- parse tree construction video -->\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=vo8HBQDi6bk\" target=\"_blank\">\n",
    "<img style='border:1px solid #000000' src=\"movie.jpg\" width=\"120\" height=\"90\" />\n",
    "</a>\n",
    "\n",
    "<!--\n",
    "\\index{parser}\n",
    "\\index{parse tree}\n",
    "-->\n",
    "An interesting and very desirable characteristic of parse trees is that they make the grouping of the symbols in an input stream\n",
    "structurally explicit.\n",
    "In Figure 1  we can see that `y * z` are grouped together as an expression and this expression together\n",
    "with the symbols `x` and `+` forming another expression.\n",
    "This hierarchical grouping of symbols, words, and phrases in parse trees makes language processing much easier compared to processing textual representations of programs.\n",
    "\n",
    "Another observation is that the same parse tree is obtained regardless whether we use a left-most or right-most derivation of\n",
    "our sentence.  Try to prove to yourself that a right-most derivation of our sentence gives rise to the same parse tree\n",
    "(the first three steps in the right-most derivation are obviously the same as in the left-most derivation above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example: The Exp0 Language\n",
    "\n",
    "<!--\n",
    "\\index{Exp0}\n",
    "\\index{Lisp}\n",
    "-->\n",
    "\n",
    "Our first programming language  is a calculator language that allows you to evaluate, store, and print the values of simple expressions -\n",
    "let us call it Exp0.\n",
    "In order to keep it simple we only allow three variable names (`x`, `y`, and `z`) and our\n",
    "numbers are limited to single digits.\n",
    "The expressions in this language are in prefix format, *e.g.*, the expression `(+ x 1)` means add the value `1` to the\n",
    "the value stored in variable `x`.\n",
    "If you know Lisp then this should look very familiar; in Lisp all\n",
    "expressions as well as statements are given in this prefix format.\n",
    "For the sake of simplicity we also only allow additive expressions, that is, we only allow addition and subtraction.\n",
    "To print a value to the terminal we write the command `p` followed by an expression.\n",
    "For example, the statement `p (+ x 1)`\n",
    "adds the value one to the value stored in `x`  and then prints that new value to the terminal.\n",
    "We use the command `s` followed by a variable name followed by an expression to store the value of\n",
    "the expression in the variable.\n",
    "Here, the statement `s y (- 9 3)` stores the value six in the variable `y`.\n",
    "\n",
    "<!--\n",
    "\\index{Exp0 grammar}\n",
    "-->\n",
    "\n",
    "Here is the grammar that defines the syntax of Exp0,\n",
    "```\n",
    "prog : stmt_list\n",
    "\n",
    "stmt_list : stmt stmt_list\n",
    "          | \"\"\n",
    "          \n",
    "stmt : p exp ;\n",
    "     | s var exp ;\n",
    "     \n",
    "exp : + exp exp\n",
    "    | - exp exp\n",
    "    | ( exp )\n",
    "    | var\n",
    "    | num\n",
    "\t\n",
    "var : x | y | z\n",
    "\n",
    "num : 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |9\n",
    "```\n",
    "The non-terminal `prog` is the start symbol.  The first rule defines the start symbol and states that programs are statement lists.\n",
    "The second and the third rules define statement lists.\n",
    "The rules for the non-terminal `stmt` define the structure of print and store commands.\n",
    "Note the semicolon that terminates these statements.  These semicolons belong to the Exp0 language we are defining.\n",
    "The rules for the non-terminal `exp` specify the structure of possibly parenthesized prefix expressions.\n",
    "As is common, variable names and numbers are also considered expressions.\n",
    "Finally, the last two sets of rules\n",
    "define our variable names and our numbers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center>\n",
    "<img src=\"figures/chap02/2/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 2. A parse tree for the Exp0 program `s x 1; p (+ x 1);`.\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use our Exp0 grammar to derive the program\n",
    "```\n",
    "s x 1 ;  p (+ x 1) ;\n",
    "```\n",
    "In this program we first store the value one in variable `x` and then we \n",
    "print out the sum of the value stored in `x` and the value one.\n",
    "Our left-most derivation starts with the start symbol `prog`,\n",
    "```\n",
    "prog =>\n",
    "stmt_list =>\n",
    "stmt  stmt_list =>\n",
    "s var exp ; stmt_list =>\n",
    "s x exp ; stmt_list =>\n",
    "s x num ; stmt_list =>\n",
    "s x 1 ; stmt_list =>\n",
    "s x 1 ; stmt stmt_list=>\n",
    "s x 1 ; p exp ; stmt_list =>\n",
    "s x 1 ; p ( exp ) ; stmt_list =>\n",
    "s x 1 ; p ( + exp exp ) ; stmt_list =>\n",
    "s x 1 ; p ( + var exp ) ; stmt_list =>\n",
    "s x 1 ; p ( + x exp ) ; stmt_list =>\n",
    "s x 1 ; p ( + x num ) ; stmt_list =>\n",
    "s x 1 ; p ( + x 1 ) ; stmt_list =>\n",
    "s x 1 ; p ( + x 1 ) ; \n",
    "```\n",
    "The recursive nature of rules 2 and 3 of our Exp0 grammar allows us to derive sequences of statements\n",
    "that make up a program.\n",
    "In the last derivation step we use the rule `stmt_list : \"\"` to let the non-terminal \"disappear\".\n",
    "The parse tree for this derivation appears in Figure 2.\n",
    "You should convince yourself that this is indeed the parse tree for the above derivation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers\n",
    "\n",
    "<!--\n",
    "\\index{parser}\n",
    "\\index{derivation}\n",
    "\\index{valid sentence}\n",
    "-->\n",
    "\n",
    "\n",
    "In the previous sections we used our intuition to select grammar rules that make up a derivationa that prove that some sentence is valid in the language of a grammar.\n",
    "Here we investigate the algorithmic construction of derivations for sentences in a particular grammar.\n",
    "Programs that perform the construction of derivations are called parsers.\n",
    "We begin our discussion with the top-down parsing algorithm.\n",
    "\n",
    "\n",
    "## Top-Down Parsing\n",
    "\n",
    "It is highly likely that when you construct the derivation of a particular sentence you first \n",
    "scan the structure of the sentence itself and then you scan\n",
    "the right sides of the grammar rules. \n",
    "Once you have an overview of the structure of the right sides of the grammar rules you will most likely pick  rules that match some\n",
    "structure in the sentence that you want to derive.  \n",
    "You keep doing this until you derive the terminals that make up the sentence.\n",
    "\n",
    "<!--\n",
    "\\index{lookahead set}\n",
    "-->\n",
    "\n",
    "Top-down parsing mimics this approach.\n",
    "However, in order to make this workable algorithmically we need to extend grammars with an additional data structure called *lookahead sets*.\n",
    "Our Exp0 grammar extended with lookahead sets looks like this,\n",
    "```\n",
    "prog : {p,s} stmt_list\n",
    "\n",
    "stmt_list : {p,s} stmt stmt_list\n",
    "          | {\"\"} \"\"\n",
    "          \n",
    "stmt : {p} p exp ;\n",
    "     | {s} s var exp ;\n",
    "     \n",
    "exp : {+} + exp exp\n",
    "    | {-} - exp exp\n",
    "    | {(} ( exp )\n",
    "    | {x,y,z} var\n",
    "    | {0,1,2,3,4,5,6,7,8,9} num\n",
    "\t\n",
    "var : {x} x | {y} y | {z} z\n",
    "\n",
    "num : {0} 0 | {1} 1 | {2} 2 | {3} 3 | {4} 4 | {5} 5 | {6} 6 | {7} 7 | {8} 8 | {9} 9\n",
    "```\n",
    "Here the lookahead sets appear between the curly braces for each rule.  \n",
    "\n",
    "<!--\n",
    "\\index{lookahead pointer}\n",
    "-->\n",
    "\n",
    "From the point of view of top-down parsing the lookahead sets summarize the structure\n",
    "of the right side of each grammar rules and they are used  in conjunction with \n",
    "a lookahead pointer to make choices between competing rules during the construction of a derivation.\n",
    "The  lookahead pointer points to the current character in the input stream.\n",
    "Consider the following input stream:\n",
    "```\n",
    "<p> + 1 2 ; \\eof\n",
    "```\n",
    "The angle brackets around the `p` character represent the lookahead pointer.\n",
    "Now consider the left-most derivation of this program using our grammar extended with the lookahead sets.\n",
    "In the derivation below we show the input stream, the derivation in the square brackets,\n",
    "and the rule which we apply.\n",
    "We start with the character under the lookahead pointer in the stream and with our start symbol `prog`.\n",
    "We can observe that the lookahead symbol matches the lookahead set for the rule defining `prog`\n",
    "```\n",
    "<p> + 1 2 ; \\eof    [<prog>]                     prog : {p,s} stmt_list =>\n",
    "<p> + 1 2 ; \\eof    [<stmt_list>]                stmt_list : {p,s} stmt stmt_list =>\n",
    "<p> + 1 2 ; \\eof    [<stmt> stmt_list]           stmt : {p} p exp ; =>\n",
    "<p> + 1 2 ; \\eof    [<p> exp ; stmt_list]        <match input character> =>\n",
    "p <+> 1 2 ; \\eof    [p <exp> ; stmt_list]        exp : {+} + exp exp =>\n",
    "p <+> 1 2 ; \\eof    [p <+> exp exp ; stmt_list]  <match input character> =>\n",
    "p + <1> 2 ; \\eof    [p + <exp> exp ; stmt_list]  exp : {0,1,2,3,4,5,6,7,8,9} num =>\n",
    "p + <1> 2 ; \\eof    [p + <num> exp ; stmt_list]  num : {1} 1 =>\n",
    "p + <1> 2 ; \\eof    [p + <1> exp ; stmt_list]    <match input character> =>\n",
    "p + 1 <2> ; \\eof    [p + 1 <exp> ; stmt_list]    exp : {0,1,2,3,4,5,6,7,8,9} num =>\n",
    "p + 1 <2> ; \\eof    [p + 1 <num> ; stmt_list]    num : {2} 2 =>\n",
    "p + 1 <2> ; \\eof    [p + 1 <2> ; stmt_list]      <match input character> =>\n",
    "p + 1 2 <;> \\eof    [p + 1 2 <;> stmt_list]      <match input character> =>\n",
    "p + 1 2 ; <\\eof>    [p + 1 2 ; <stmt_list>]      stmt_list : \"\" =>\n",
    "p + 1 2 ; <\\eof>    [p + 1 2 ;] \n",
    "```\n",
    "At the final step of the derivation the lookahead pointer points to the `\\eof` symbol, there are no non-terminals left in the derivation, and the derived sentence matches\n",
    "the program in the input stream: We say that we successfully parsed the input stream.\n",
    "\n",
    "<!-- TODO: more detailed explanation of the derivation -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center>\n",
    "<img src=\"figures/chap02/3/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 3. A parse tree for the Exp0 program `p + 1 2;`.\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{top-down parsing}\n",
    "-->\n",
    "\n",
    "The parse tree for our derivation is shown in Figure 3.\n",
    "If you look carefully at the derivation then you can see the it constructs the elements of the parse tree starting\n",
    "with the root not.\n",
    "Therefore his is called top-down parsing.\n",
    "\n",
    "<!--\n",
    "\\index{LL(1)}\n",
    "\\index{LL(1)!parsing}\n",
    "\\index{LL(1)!grammar}\n",
    "-->\n",
    "\n",
    "Another name for this approach to parsing is *LL(1)* parsing.  The first L stands for the fact that we are reading the input stream\n",
    "from left to right.  The second L indicates that we are performing a left-most derivation, and (1) means that we are using a\n",
    "single lookahead symbol in the input stream\n",
    "\n",
    ">Top-down parsing means building a derivation or a parse tree starting with the start symbol.  Another name for this is LL(1) parsing which means\n",
    "reading from the (L)eft constructing the (L)eft-most derivation using (1) lookahead symbol. \n",
    "Grammars that allow us to do LL(1) parsing are called LL(1) grammars.\n",
    "\n",
    "The key to LL(1) parsing are the lookahead sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookahead Sets\n",
    "\n",
    "<!--\n",
    "\\index{lookahead set}\n",
    "-->\n",
    "\n",
    "You probably noticed that the introduction of the lookahead pointer and the lookahead sets made constructing derivations very mechanical.\n",
    "There is no guess work in terms of which rule to apply when constructing a derivation.\n",
    "This is just what we need in order to have machines do this.\n",
    "\n",
    "The first step in mechanizing this whole process is the construction of the lookahead set.\n",
    "Here is the Python code that given a grammar encoded as an appropriate data structure will\n",
    "generate the lookahead set for each of the rules of the grammar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from grammar_stuff import first_symbol, terminal_set, non_terminal_set\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_lookahead_sets(G):\n",
    "    '''\n",
    "    Accepts: G is a context-free grammar viewed as a list of rules\n",
    "    Returns: GL is a context-free grammar extended with lookahead sets\n",
    "    '''\n",
    "    GL = []\n",
    "    for R in G:\n",
    "        (A, rule_body) = R\n",
    "        S = first_symbol(rule_body) \n",
    "        if S == \"\":\n",
    "            GL.append((A, set([\"\"]), rule_body)) \n",
    "        elif S in terminal_set(G):\n",
    "            GL.append((A, set(S), rule_body)) \n",
    "        elif S in non_terminal_set(G):\n",
    "            L = lookahead_set(S,G)\n",
    "            GL.append((A, L, rule_body))\n",
    "    return GL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates over the rules of a grammar and looks at the first symbol of each rule body.\n",
    "If the first symbol is either the empty string or a terminal symbol the function just adds the first symbol\n",
    "as the lookahead set for that rule.  If the first symbol of the rule is a non-terminal then the function computes\n",
    "the lookahead set for that non-terminal with the function `lookahead_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lookahead_set(N, G):\n",
    "    '''\n",
    "    Accepts: N is a non-terminal in G\n",
    "    Accepts: G is a context-free grammar\n",
    "    Returns: L is a lookahead set\n",
    "    '''\n",
    "    L = set()\n",
    "    for R in G:\n",
    "        (A, rule_body) = R\n",
    "        if A == N:\n",
    "            Q = first_symbol(rule_body)\n",
    "            if Q == \"\":\n",
    "                raise ValueError(\"non-terminal {} is a nullable prefix\".format(A))\n",
    "            elif Q in terminal_set(G):\n",
    "                L = L | set(Q)\n",
    "            elif Q in non_terminal_set(G):\n",
    "                L = L | lookahead_set(Q, G)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `lookahead_set` iterates over rules of the grammar `G` that have the non-terminal `N` on the\n",
    "left side.  If the first symbol of the rule body of one of these rules is the empty string then we abort.\n",
    "Our simple algorithm cannot deal with nested rules that all derive the empty string.\n",
    "On the other hand, if the first symbol is a terminal then we add this terminal to the lookahead set.  If the\n",
    "first symbol is a non-terminal then we call ourselves recursively in order to determine the lookahead set for that\n",
    "non-terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these functions using the following grammar G:\n",
    "```\n",
    "exp : + exp exp\n",
    "    | - exp exp\n",
    "    | x\n",
    "    | y\n",
    "    | z\n",
    "```\n",
    "We encode the grammar in Python as a list of tuples where each tuple represents a grammar rule in form:\n",
    "(left side, right side). The right side is a list of symbols occuring in the right side of that grammar rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = [('exp',['+','exp','exp']), \n",
    "     ('exp',['-','exp','exp']), \n",
    "     ('exp',['x']), \n",
    "     ('exp',['y']), \n",
    "     ('exp',['z'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the terminals and non-terminals of our grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y', 'x', 'z', '+', '-'}\n"
     ]
    }
   ],
   "source": [
    "print(terminal_set(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp'}\n"
     ]
    }
   ],
   "source": [
    "print(non_terminal_set(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the extended grammar with the lookahead sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('exp', {'+'}, ['+', 'exp', 'exp']),\n",
      " ('exp', {'-'}, ['-', 'exp', 'exp']),\n",
      " ('exp', {'x'}, ['x']),\n",
      " ('exp', {'y'}, ['y']),\n",
      " ('exp', {'z'}, ['z'])]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(compute_lookahead_sets(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left-Recursive Grammars are not LL(1)\n",
    "\n",
    "<!--\n",
    "\\index{left-recursive grammar}\n",
    "\\index{mutual left-recursion} \n",
    "-->\n",
    "\n",
    "\n",
    "There are grammars for which it is impossible to construct lookahead sets.\n",
    "Consider the following grammar,\n",
    "```\n",
    "exp : exp + exp\n",
    "    | x\n",
    "    | y\n",
    "```\n",
    "Our lookahead set algorithm will recurse indefinitely on the first rule of this grammar (convince yourself!).\n",
    "The indefinite recursion is due to the fact that the first non-terminal on the right side of the rule is exactly the \n",
    "same non-terminal on the left side of the rule.  This is called *left-recursion* and grammars that exhibit left-recursion in their rules cannot be used for LL(1) parsing. We say,\n",
    "\n",
    "> Left-recursive grammars are not LL(1).\n",
    "\n",
    "You have to be careful, left-recursion is not always immediately apparent.  Consider the following grammar with\n",
    "mutually recursive rules,\n",
    "```\n",
    "exp : term + var\n",
    "    | var\n",
    "    \n",
    "term : exp + var\n",
    "     | var\n",
    "     \n",
    "var : x\n",
    "    | y\n",
    "```\n",
    "None of the rules in this grammar is immediately left-recursive.  However, the mutual left-recursion of the first and third rule\n",
    "is problematic and again our lookahead set algorithm will recurse indefinitely showing that this grammar is not LL(1).\n",
    "Later on we will see techniques that allow us to rewrite left-recursive grammars into grammars that are LL(1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Grammars that are not LL(1)\n",
    "\n",
    "<!--\n",
    "\\index{rule!prefix}\n",
    "\\index{rule!factoring}\n",
    "-->\n",
    "\n",
    "There is another class of grammars that is not considered to be LL(1); grammars where the prefixes of the right\n",
    "sides of rules overlap.\n",
    "Consider this grammar,\n",
    "```\n",
    "exp : + exp exp\n",
    "    | term\n",
    "    \n",
    "term : var [ exp ]\n",
    "     | var\n",
    "     \n",
    "var : x | y\n",
    "```\n",
    "Notice that in this grammar the two rules defining the non-terminal `term` both start with the non-terminal `var`; we say that the two rules share a *prefix*.\n",
    "This is a problem because both rules will have the same lookahead set which prevents LL(1) parsing from being able to\n",
    "choose a rule unambiguously. We say,\n",
    "\n",
    ">Grammars that have rules defining the same non-terminal and that share a common prefix are not LL(1).\n",
    "\n",
    "Turns out there is an easy fix.  We can rewrite the grammar above as follows,\n",
    "```\n",
    "exp : + exp exp\n",
    "    | term\n",
    "    \n",
    "term : var array\n",
    "\n",
    "array : [ exp ]\n",
    "      | \"\"\n",
    "      \n",
    "var : x | y\n",
    "```\n",
    "This technique is called *rule factoring* and is often used to turn non-LL(1) grammars into LL(1) grammars.\n",
    "\n",
    "<!--\n",
    "\\index{nullable prefix}\n",
    "-->\n",
    "\n",
    "There is one additional class of grammars that are not considered LL(1) according to our lookahead set computation.\n",
    "These are grammars that have rules with right sides that start with non-terminals deriving the empty string.\n",
    "Consider the following grammar,\n",
    "```\n",
    "A : B a\n",
    "  | c\n",
    "  \n",
    "B : b\n",
    "  | \"\"\n",
    "```\n",
    "Without the test  in our algorithm rejecting these kinds of grammars our lookahead set would look something like this for the grammar above,\n",
    "```\n",
    "A : {b,\"\"} B a\n",
    "  | {c} c\n",
    "  \n",
    "B : {b} b\n",
    "  | {\"\"} \"\"\n",
    "```\n",
    "Here the empty string as a lookahead symbol for the first rule does not make any sense because the right side of that rule\n",
    "is not the empty string and therefore we reject this grammar.\n",
    "We could of course extend out lookahead set algorithm to accommodate this empty string and compute the\n",
    "lookahead set for the first rule as `{ a, b}` but the algorithm that does this in the general case\n",
    "is too complicated for our purposes considering that there is an easy rewrite of the grammar,\n",
    "```\n",
    "A : b a\n",
    "  | a\n",
    "  | c\n",
    "```\n",
    "Grammars with rules whose right sides start with non-terminals that derive the empty string are said to contain *nullable prefixes*.  Therefore, according to our definition of lookahead sets and for our purposes we say that,\n",
    "\n",
    "> In our setting grammars with nullable prefixes are not LL(1).\n",
    "\n",
    "<!--\n",
    "\\index{LL(1)!language}\n",
    "-->\n",
    "\n",
    "The fact that we can have multiple grammars that define the same language is interesting in itself.  The fact that some of these grammars are LL(1)\n",
    "and some are not is even more interesting and leads to the following observation,\n",
    "\n",
    "> A language is called LL(1) if it has at least one LL(1) grammar.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Top-Down Parsing Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{top-down parsing}\n",
    "-->\n",
    "\n",
    "Now that we know how to construct grammars that have  lookahead sets we can examine a simple top-down parsing algorithm itself.\n",
    "The algorithm takes an input stream and a context-free grammar extended with lookahead sets as inputs and returns `True` or `False`\n",
    "depending on whether the program in the input stream is a valid sentence in language of the grammar or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from grammar_stuff import terminal_set, non_terminal_set, start_symbol, find_matching_rule\n",
    "from grammar_stuff import Stack, InputStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topdown_parse(I, GL):\n",
    "    # Accepts: I is an input stream \n",
    "    # Accepts: GL is an LL(1) grammar extended with lookahead sets  \n",
    "    # Returns: True or False\n",
    "    S = Stack()\n",
    "    S.push(start_symbol(GL))\n",
    "    while not S.empty():\n",
    "        P = I.pointer()\n",
    "        A = S.peek()\n",
    "        if A in non_terminal_set(GL):\n",
    "            rule = find_matching_rule(GL, A, P)\n",
    "            if not rule:\n",
    "                return False\n",
    "            else:\n",
    "                S.pop()\n",
    "                (A, L, rule_body) = rule\n",
    "                S.push_reverse(rule_body)\n",
    "        elif A in terminal_set(GL):\n",
    "            if A == P:\n",
    "                S.pop()\n",
    "                I.next()\n",
    "            elif A == \"\":\n",
    "                S.pop()\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    if I.end_of_file():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally the algorithm uses a stack to keep track of the derivations and essentially mechanizes the derivation we\n",
    "constructed in section 'Top-Down Parsing'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our parsing algorithm on our simple grammar from before,\n",
    "```\n",
    "exp : + exp exp\n",
    "    | - exp exp\n",
    "    | x\n",
    "    | y\n",
    "    | z\n",
    "```\n",
    "As before, we encode our grammar as a list of tuples where each tuple represents a rule,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = [('exp',['+','exp','exp']), \n",
    "     ('exp',['-','exp','exp']), \n",
    "     ('exp',['x']), \n",
    "     ('exp',['y']), \n",
    "     ('exp',['z'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up our input stream and compute the grammar `GL` which is the grammar `G` enriched with the\n",
    "lookahead sets,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I = InputStream(['+', 'x', 'y'])\n",
    "\n",
    "GL = compute_lookahead_sets(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to parse our input stream `I` using the extended grammar `GL`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(topdown_parse(I, GL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! It is clear that the input stream represents a valid sentence and parsing it confirms it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this with our Exp0 language.  Here is the grammar of Exp0 rewritten slightly so that our\n",
    "lookahead set algorithm can process it,\n",
    "```\n",
    "prog : stmt prog\n",
    "     | \"\"\n",
    "          \n",
    "stmt : p exp ;\n",
    "     | s var exp ;\n",
    "     \n",
    "exp : + exp exp\n",
    "    | - exp exp\n",
    "    | ( exp )\n",
    "    | var\n",
    "    | num\n",
    "\t\n",
    "var : x | y | z\n",
    "\n",
    "num : 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |9\n",
    "```\n",
    "Encoding this grammar as a tuple list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = [\n",
    "    ('prog',['stmt','prog']),\n",
    "    ('prog',[\"\"]),\n",
    "    ('stmt',['p','exp',';']),\n",
    "    ('stmt',['s','var','exp',';']),\n",
    "    ('exp',['+','exp','exp']),\n",
    "    ('exp',['-','exp','exp']),\n",
    "    ('exp',['(','exp',')']),\n",
    "    ('exp',['var']),\n",
    "    ('exp',['num']),\n",
    "    ('var',['x']),\n",
    "    ('var',['y']),\n",
    "    ('var',['z']),\n",
    "    ('num',['0']),\n",
    "    ('num',['1']),\n",
    "    ('num',['2']),\n",
    "    ('num',['3']),\n",
    "    ('num',['4']),\n",
    "    ('num',['5']),\n",
    "    ('num',['6']),\n",
    "    ('num',['7']),\n",
    "    ('num',['8']),\n",
    "    ('num',['9'])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the lookahead sets,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prog', {'s', 'p'}, ['stmt', 'prog']),\n",
      " ('prog', {''}, ['']),\n",
      " ('stmt', {'p'}, ['p', 'exp', ';']),\n",
      " ('stmt', {'s'}, ['s', 'var', 'exp', ';']),\n",
      " ('exp', {'+'}, ['+', 'exp', 'exp']),\n",
      " ('exp', {'-'}, ['-', 'exp', 'exp']),\n",
      " ('exp', {'('}, ['(', 'exp', ')']),\n",
      " ('exp', {'y', 'z', 'x'}, ['var']),\n",
      " ('exp', {'2', '4', '0', '3', '5', '6', '7', '1', '9', '8'}, ['num']),\n",
      " ('var', {'x'}, ['x']),\n",
      " ('var', {'y'}, ['y']),\n",
      " ('var', {'z'}, ['z']),\n",
      " ('num', {'0'}, ['0']),\n",
      " ('num', {'1'}, ['1']),\n",
      " ('num', {'2'}, ['2']),\n",
      " ('num', {'3'}, ['3']),\n",
      " ('num', {'4'}, ['4']),\n",
      " ('num', {'5'}, ['5']),\n",
      " ('num', {'6'}, ['6']),\n",
      " ('num', {'7'}, ['7']),\n",
      " ('num', {'8'}, ['8']),\n",
      " ('num', {'9'}, ['9'])]\n"
     ]
    }
   ],
   "source": [
    "GL = compute_lookahead_sets(G)\n",
    "pp.pprint(GL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's set up the input stream for the program `p + 1 2 ;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I = InputStream(['p','+','1','2',';'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a valid sentence in the language of the grammar `G`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(topdown_parse(I, GL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4 shows a trace of our top-down parsing algorithms at work on the example that we just parsed.\n",
    "The figure shows the stack, the input stream with the stream pointer shown in angle brackets, and the rules\n",
    "as they are being applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center>\n",
    "<img src=\"figures/chap02/4/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 4. A trace of our top-down parsing algorithm  using the extended grammar `GL` for the Exp0 language\n",
    " the input stream `p + 1 2 ; \\eof`.\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this derivation to the derivation that we constructed by hand for this input stream in section 'Top-Down Parsing'.  You will find them virtually indentical with the exception that we had to make some changes to the grammar to accommodate our lookahead set computation.  That means our top-down parsing algorithm mechanizes our derivations\n",
    "very nicely.\n",
    "\n",
    "<!-- TODO patch up the movie for this - videos/chap02/q2 -->\n",
    "\n",
    "The following is an animation of the derivation process using our algorithm.\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=DAnlgrbSpyQ\" target=\"_blank\">\n",
    "<img style='border:1px solid #000000' src=\"movie.jpg\" width=\"120\" height=\"90\" />\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom-Up Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{bottom-up parser}\n",
    "-->\n",
    "\n",
    "In the previous sections we have looked at top-down parsers which conceptually build parse trees top-down starting at the root.\n",
    "As you might guess, bottom-up parsers do exactly the opposite, rather than building a parse tree starting from the root, a bottom-up parser\n",
    "constructs a parse tree starting with the leaves of the parse tree working its way up to the root node. \n",
    "A bottom-up parser also accepts an input stream `I` and a grammar `G` as arguments and uses\n",
    "a stack `S` internally for processing.\n",
    "However, it is worthwhile pointing out that in this case the grammar does not have to be a LL(1) grammar and we do not have to compute\n",
    "the lookahead sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{reduce action}\n",
    "\\index{shift action}\n",
    "\\index{accept action}\n",
    "\\index{reject action}\n",
    "-->\n",
    "\n",
    "Traditionally bottom-up parsers are understood in terms of four different actions,\n",
    "\n",
    "1. **Reduce**: Apply a grammar rule to the stack.\n",
    "2. **Shift**: Push a new symbol from the input stream onto the stack\n",
    "3. **Accept**: Indicate that the string in the input stream is a valid sentence in language of the grammar.\n",
    "4. **Reject**: Indicate that the string in the input stream is not a valid sentence in the language of the grammar.\n",
    "\n",
    "The following algorithm shows these actions as comments for the different pieces of code\n",
    "accomplishing the respective tasks.\n",
    "If you look at the code carefully then you will notice that the algorithm uses the rules of the grammar backwards ('Reduce') compared to what we\n",
    "saw in the top-down parser.\n",
    "Here the parse will try to match the right side of the rule with the contents on the top of the stack.  If successful\n",
    "then the stack is popped and the corresponding non-terminal is pushed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Bottom-Up Parsing Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a stack based bottom up parsing algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from grammar_stuff import right_side_match, start_symbol\n",
    "from grammar_stuff import Stack, InputStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bottomup_parse(I, G):\n",
    "    # Accepts: I is an input stream\n",
    "    # Accepts: G is a context-free grammar\n",
    "    # Returns: True or False\n",
    "    S = Stack()\n",
    "    while True:\n",
    "        rule = right_side_match(G, S)\n",
    "        if rule: # Reduce\n",
    "            (Q, rule_body) = rule\n",
    "            S.popn(len(rule_body))\n",
    "            S.push(Q)\n",
    "        elif not I.end_of_file(): # Shift\n",
    "            S.push(I.pointer())\n",
    "            I.next()\n",
    "        elif S.peek() == start_symbol(G) and I.end_of_file(): # Accept\n",
    "            return True\n",
    "        else: # Reject\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our parser on a grammar that our top-down approach could not handle: the left-recursive grammar\n",
    "that specifies simple expressions,\n",
    "```\n",
    "exp : exp + exp\n",
    "    | x\n",
    "    | y\n",
    "```\n",
    "We encode this grammar as our familiar list of tuples,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = [\n",
    "    ('exp',['exp','+','exp']),\n",
    "    ('exp',['x']),\n",
    "    ('exp',['y'])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to set up our input stream with the program `x + y`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I = InputStream(['x','+','y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse this stream,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(bottomup_parse(I, G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! As expected the program `x + y` does belong to the language of the grammar.\n",
    "\n",
    "Let's look at a trace of this parsing algorithm parsing the input stream `x + y \\eof`.  Figure 5 shows this trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center>\n",
    "<img src=\"figures/chap02/5/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 5. A trace of our bottom-up parsing algorithm  for the input stream `x + y \\eof`.\n",
    "</center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{right-most derivation}\n",
    "\\index{LR(0) parser}\n",
    "-->\n",
    "\n",
    "A scan of the Rule column of Figure5  reveals a couple of interesting things:\n",
    "\n",
    "1. The algorithm seems to be using the rules of the grammar in exactly the opposite order compared to the top-down parsing algorithm.\n",
    "\n",
    "2. It is not difficult to see that this algorithm constructs the parse tree from the bottom up: it first constructs two subtrees using the rules `exp : x` and `exp : y` and then combines these two subtrees into a single tree using the rule `exp : exp + exp`.\n",
    "\n",
    "3. If we scan the Stack column from bottom to top we can see that the algorithm constructs a right-most derivation,\n",
    "```\n",
    "exp =>\n",
    "exp + exp =>\n",
    "exp + y =>\n",
    "x + y\n",
    "```\n",
    "\n",
    "This parsing algorithm did not use the lookahead pointer in order to make decisions\n",
    "as to which rules to apply to the stack.  It simply used the lookahead pointer as a way to remember where it is in the input stream.\n",
    "Because it does not use the lookahead pointer to make parsing decisions we say that the algorithm uses (0) lookahead symbols.\n",
    "Also note that the algorithm reads the input stream from the left but constructs the right-most derivation.\n",
    "We can now say the following,\n",
    "\n",
    "> Bottom-up parsers build parse trees starting from the leaves and work their way towards the root node.\n",
    "This is also called LR parsing.\n",
    "In our case we have LR(0) parsing because we are reading from the (L)eft constructing the (R)ight-most\n",
    "derivation using (0) lookahead symbols.\n",
    "Grammars that allow us to do LR(0) parsing are called LR(0) grammars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an animation of our bottom-up parsing algorithm.\n",
    "<!-- videos/chap02/q3 -->\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=tC7allwJZlM\" target=\"_blank\">\n",
    "<img style='border:1px solid #000000' src=\"movie.jpg\" width=\"120\" height=\"90\" />\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Closer Look at LR(0)\n",
    "\n",
    "Our LL(1) and LR(0) parsing algorithms differ in the kind of grammars the algorithms can use for parsing input streams.  \n",
    "We already saw that the grammar,\n",
    "```\n",
    "exp : exp + exp\n",
    "    | x\n",
    "    | y\n",
    "```\n",
    "is not LL(1) because we cannot construct the lookahead sets for this grammar.  But we just proved that it is\n",
    "LR(0) because we were able to use it in our bottom-up parsing algorithm.\n",
    "You might be asking at this point: are there LL(1) grammars that are not LR(0)? The answer is yes and surprisingly\n",
    "the grammar for our Exp0 language,\n",
    "```\n",
    "prog : stmt prog\n",
    "     | \"\"\n",
    "          \n",
    "stmt : p exp ;\n",
    "     | s var exp ;\n",
    "     \n",
    "exp : + exp exp\n",
    "    | - exp exp\n",
    "    | ( exp )\n",
    "    | var\n",
    "    | num\n",
    "\t\n",
    "var : x | y | z\n",
    "\n",
    "num : 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |9\n",
    "```\n",
    "is LL(1) as we have shown in the previous sections but not LR(0).\n",
    "This grammar is not LR(0) due mainly to rule,\n",
    "```\n",
    "prog : \"\"\n",
    "```\n",
    "which states that program can be empty.  In bottom-up parsing without lookahead it would be very \n",
    "difficult to apply this rule in reverse.  We have two options: (a) either it can be applied anywhere because the empty string can be \n",
    "rewritten into `prog` at any time during the derivation or (b) it can never be applied because the empty string\n",
    "never occurs on the stack explicitely.\n",
    "It doesn't matter which option we adopt the outcome would be an illegal derivation sequence\n",
    "and therefore grammars with rules that have empty right sides are not LR(0).\n",
    "However, it can be shown that grammars with rules that have empty right sides are LR(1), that is, \n",
    "using a lookahead allows us to determine whether it is legal to reduce an empty string to the appropriate\n",
    "non-terminal.\n",
    "\n",
    "> Grammars that have rules with empty right sides are not LR(0).\n",
    "\n",
    "<!--\n",
    "\\index{reduce-reduce conflict}\n",
    "-->\n",
    "\n",
    "Grammars that have rules with the same right sides are also not considered LR(0).\n",
    "Consider the following grammar snippet,\n",
    "```\n",
    "prog : stmt stmt_list\n",
    "stmt_list : stmt stmt_list\n",
    "```\n",
    "and also consider the following stack configuration,\n",
    "```\n",
    "\\eos stmt stmt_list\n",
    "```\n",
    "At this point our bottom-up parsing algorithm is stuck because it has no way of knowing which rule to pick.\n",
    "The right sides of both rules match the top of the stack and the algorithm has no way of knowing which rule to \n",
    "pick to reduce the stack.\n",
    "In the terminology of LR(0) parsers this is called a *reduce-reduce* conflict.\n",
    "Reduce-reduce conflicts show up in grammars with rules that have exactly the same right sides.\n",
    "We say that,\n",
    " \n",
    "> Grammars with reduce-reduce conflicts are not LR(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center>\n",
    "<img src=\"figures/chap02/6/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 6. A trace of the bottom-up parsing algorithm for the input stream `if e then p else q \\eof`.\n",
    "</center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{shift-reduce conflict}\n",
    "-->\n",
    "\n",
    "Just as in the case of top-down parsing, grammars with rules whose right sides share common prefixes are not considered LR(0) grammars. \n",
    "Consider this grammar snippet,\n",
    "```\n",
    "stmt : if exp then stmt\n",
    "     | if exp then stmt else stmt\n",
    "     | p\n",
    "     | q\n",
    "     \n",
    "exp : e\n",
    "```\n",
    "Figure 6 is a trace of the bottom-up parsing algorithm using this grammar snippet together with the input stream,\n",
    "```\n",
    "if e then p else q \\eof\n",
    "```\n",
    "In the trace the steps 1 through 6 are straightforward with the exception that here we grouped symbols together to make up keywords\n",
    "and we move the lookahead pointer from input symbol to symbol.\n",
    "The problem arises in step 7: we can certainly apply the first grammar rule to reduce the top of the stack but then we are stuck with \n",
    "a fragment of the if-then-else statement in the input stream for which there are no rules to parse it any further.\n",
    "The other alternative is to shift the `else` symbol onto the stack and continue parsing.\n",
    "The is called a *shift-reduce* conflict and we say that,\n",
    "\n",
    "> A grammar that exhibits shift-reduce conflicts is not LR(0).\n",
    "\n",
    "We could change the behavior of our algorithm in such a way that if it detects a shift-reduce conflict it always makes the decision to shift\n",
    "thus avoiding leaving snippets of syntax unused and unparsable in the input stream.\n",
    "This is in fact what most industrial strength bottom-up parsers do, they issue a warning about a shift-reduce conflict and resolve the conflict\n",
    "by always shifting.\n",
    "On the other hand, there is an easy way to rewrite this grammar so that it becomes LR(0) by factoring it.\n",
    "\n",
    "Similar to LL(1) languages we find that there are multiple grammars that define a language, some of the grammars are LR(0) some are not.  \n",
    "We can assert the following,\n",
    "\n",
    "> We call a language LR(0) if it has at least one LR(0) grammar.\n",
    "\n",
    "<!--\n",
    "\\index{rule postfix}\n",
    "-->\n",
    "We should mention that technically LR(0) parsers can run into another kind of reduce-reduce conflict when\n",
    "the right side of a rule is a postfix of the right side of another rule.  \n",
    "Consider the grammar,\n",
    "```\n",
    "list : list item\n",
    "     | item\n",
    "     \n",
    "item : e\n",
    "```\n",
    "Here the right side of the second rule is a postfix to the right side of the first rule.\n",
    "When parsing the input stream ` e e \\eof` with this grammar a pure LR(0) parser has to make a choice when parsing the second\n",
    "`e` whether to apply the first rule or the second rule in order to reduce the stack to a non-terminal.\n",
    "If the parser were to choose the second rule the parse will terminate in an error.\n",
    "In an implementation of LR(0) parsers we could eliminate this choice by forcing the parser to always apply the longest right side of a rule that\n",
    "matches the stack.  Therefore, in this case the LR(0) parser will always pick the first rule in order to reduce the stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Parsers by Hand\n",
    "\n",
    "<!--\n",
    "\\index{rule application}\n",
    "-->\n",
    "If we wanted to build a parser for a language it is clear that we could simply implement either one of the parsers\n",
    "we just discussed.\n",
    "However, both of these algorithms spend a significant amount of time searching the grammars for an appropriate rule to apply.\n",
    "This kind of search is a source of inefficiency.\n",
    "\n",
    "<!--\n",
    "\\index{recursive descent parser}\n",
    "-->\n",
    "It turns out that there is a clever way of turning grammars into parsers where no searching is done whatsoever. \n",
    "In this approach to constructing a parser we turn non-terminals on the left side of rules into *function definitions* and non-terminals appearing\n",
    "within the right side of rules into *functional calls*.  The resulting parser then has a function for each non-terminal in the grammar\n",
    "and each function knows how to parse the structures associated with its non-terminal.  This kind of parser is called a *recursive descent*\n",
    "parser because it uses function recursion as a way to make progress during parsing.\n",
    "Most notably, we require the grammar used to construct a recursive descent parser \n",
    "to be an LL(1) grammar with lookahead sets.\n",
    "\n",
    "Constructing a recursive descent parser is best illustrated with an example.\n",
    "Consider the following LL(1) grammar with lookahead sets,\n",
    "```\n",
    "exp : {+} + exp exp\n",
    "    | {x,y} var\n",
    "    \n",
    "var : {x} x \n",
    "    | {y} y\n",
    "```\n",
    "This grammar defines two non-terminals, `exp` and `var`, and therefore we expect that the recursive descent parser also has two functions.\n",
    "The following is the function that parses `exp`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from grammar_stuff import InputStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exp():\n",
    "    sym = I.pointer()\n",
    "    if sym == '+':\n",
    "        I.next()\n",
    "        exp()\n",
    "        exp()\n",
    "    elif sym == 'x':\n",
    "        var()\n",
    "    elif sym == 'y':\n",
    "        var()\n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(sym))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this code carefully you will find that the function  parses expressions according to the \n",
    "first two grammar rules above: the grammar rules that define the non-terminal `exp`.\n",
    "The function first retrieves the\n",
    "symbol at the lookahead pointer in the input stream.\n",
    "Then, if this symbol matches the lookahead set of the first rule, *i.e.*, if this lookahead symbol matches the `+` symbol then we execute the first rule.\n",
    "This means first moving the lookahead pointer to the next character skipping the `+` symbol and then calling `exp()`\n",
    "recursively (twice -- according to the right side of the first rule).\n",
    "On the other hand, if the symbol from the lookahead pointer matches either `x` or `y` then we continue parsing the input with the second rule\n",
    "of the grammar by calling the function `var()`.\n",
    "If the symbol from the lookahead pointer matches none of the symbols in the lookahead sets then the input stream does not conform\n",
    "to the structure of an expression and we flag a syntax error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a similar function for the non-terminal `var`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var():\n",
    "    sym = I.pointer()\n",
    "    if sym == 'x':\n",
    "        I.next()\n",
    "    elif sym == 'y':\n",
    "        I.next()\n",
    "    else:\n",
    "        raise SyntaxError('unexpected symbol {} while parsing'.format(sym))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous function this function uses the lookahead sets to select which rule to execute.\n",
    "In this case the right sides of the rules are very simple, they simply match the variable names in the input stream.\n",
    "However, if the variable names in the input stream are not correct then the function flags an error and aborts.\n",
    "\n",
    "Consider parsing the input stream,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I = InputStream(['+','x','y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the parsing function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is a syntactically correct expression in the input stream nothing much exciting is happening.\n",
    "\n",
    "Here is a closer look at the parser.\n",
    "To start the parser we call the function associated with start symbol of our grammar, `exp`.\n",
    "This function looks at the symbol under the lookahead pointer, finds a `+` symbol, and uses the first if clause.  Here we move the lookahead pointer to the next symbol in the input stream and then call ourselves\n",
    "recursively twice.  The first time to match the `x` symbol and the second time to match the `y` symbol.\n",
    "Notice that there is no searching involved during parsing, everything is completely determined by the lookahead pointer\n",
    "and the lookahead sets in the case statements.\n",
    "One way of looking at recursive descent parsers is that the function invocation for a non-terminal activates the right sides of \n",
    "the associated rules all at the same time.\n",
    "The precise right side to execute is then determined by comparing the lookahead pointer and the lookahead set for each right side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the input stream to something that is not syntactically correct,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I = InputStream(['-','x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected symbol - while parsing (<string>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected symbol - while parsing\n"
     ]
    }
   ],
   "source": [
    "exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, the parser rejects the expression in the input stream as incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Descent is LL(1)\n",
    "\n",
    "We know that this parser uses an LL(1) grammar but is it itself an LL(1) parser?  From the discussion above we already know that\n",
    "a recursive descent parser reads the input from left to right; this gives us our first L.  We also know that it uses a single symbol lookahead\n",
    "pointer; this gives us our (1).  Finally, the right sides of the grammar rules are encoded in the recursive descent parsers just the way they\n",
    "are written in the grammar, *i.e.* in the implementation of the rule,\n",
    "```\n",
    "exp : {+} + exp exp\n",
    "```\n",
    "we first match the `+` symbol and then call the two functions for `exp`,\n",
    "```\n",
    "...\n",
    "if sym == '+':\n",
    "    I.next()\n",
    "    exp()\n",
    "    exp()\n",
    "...\n",
    "```\n",
    "Since code in languages Python (or any imperative language for that matter) is executed top to bottom and\n",
    "left to right, we know that the function for the first `exp` in the grammar rule is always executed before the second one.\n",
    "That means the recursive descent parser constructs a left-most derivation.  This gives us our second L.\n",
    "\n",
    "> Therefore, a recursive descent parser is a LL(1) parser.\n",
    "\n",
    "<!-- TODO: fix the movie for this\n",
    "{\\bookurl/b/2/q4/figure.mov}\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{language specification}\n",
    "\\index{language implementer}\n",
    "-->\n",
    "\n",
    "Very few language implementers would choose to implement a parser by hand. One reason being that common languages like Java or C typically involve hundreds of \n",
    "grammar rules and implementing those in a recursive descent parser by hand \n",
    "is a nontrivial undertaking.\n",
    "Another reason and perhaps the more important reason is that languages evolve during\n",
    "their lifecycle.  As a language matures new features are added and old features are \n",
    "modified or depreciated.\n",
    "Changing a hand-coded parser to keep up with an evolving language requires major, time consuming reengineering of \n",
    "the parser every time the specification of the language changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center>\n",
    "<img src=\"figures/chap02/7/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 7. A parser generator viewed as a translator.\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{parser generator}\n",
    "\\index{grammar specification}\n",
    "-->\n",
    "\n",
    "Most language implementers would choose to use a parser generator in order to construct parsers.\n",
    "A parser generator reads a grammar specification and generates source code for a parser that can parse\n",
    "the valid sentences in the language of the specified grammar.\n",
    "In this way we can view a parser generator as a translator for a domain specific language (see Figure 7).\n",
    "Here the domain specific language is the grammar specification language and the target language is whatever language\n",
    "the parser code is generated in.  In our case this will be Python.\n",
    "\n",
    "The parser generator will go through all the typical stages of a translator.  It first performs a syntax analysis (parsing!) of the \n",
    "grammar specification file and constructs an IR.  Then it performs the semantic analysis of the grammar file.\n",
    "Here it checks whether all non-terminals have been defined by at least one rule, for instance.  Finally, it uses the IR to generate the\n",
    "parser code.\n",
    "The advantage of using a parser generator is that as a language evolves all we need to do is to modify the grammar specification,\n",
    "the parser code is then generated from the modified grammar specification and does not need any additional engineering.\n",
    "\n",
    "<!--\n",
    "\\index{Ply}\n",
    "-->\n",
    "\n",
    "The particular parser generator we will be using throughout the rest of the  book is called \n",
    "<a href=\"http://www.dabeaz.com/ply/\">Ply</a> (Python Lex-Yacc).\n",
    "Ply generates LR(1) parsers from grammar specifications.\n",
    "The interesting part about Ply is that it is tightly integrated with the Python environment and\n",
    "generating parsers from the grammar specifications does not require any additional steps from side\n",
    "of the developer.\n",
    "\n",
    "Here is the Ply grammar specification for our Exp0 language in the file `exp0_gram.py`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load code/chap02/exp0_gram.py\n",
    "from ply import yacc\n",
    "from exp0_stuff import tokens, lexer\n",
    "\n",
    "def p_grammar(_):\n",
    "    \"\"\"\n",
    "    prog : stmt_list\n",
    "    \n",
    "    stmt_list : stmt stmt_list\n",
    "              | empty\n",
    "              \n",
    "    stmt : 'p' exp ';'\n",
    "         | 's' var exp ';'\n",
    "         \n",
    "    exp : '+' exp exp\n",
    "        | '-' exp exp\n",
    "        | '(' exp ')'\n",
    "        | var\n",
    "        | num\n",
    "        \n",
    "    var : 'x' \n",
    "        | 'y' \n",
    "        | 'z'\n",
    "        \n",
    "    num : '0' \n",
    "        | '1' \n",
    "        | '2' \n",
    "        | '3' \n",
    "        | '4' \n",
    "        | '5' \n",
    "        | '6' \n",
    "        | '7' \n",
    "        | '8' \n",
    "        | '9'\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def p_empty(p):\n",
    "    'empty :'\n",
    "    pass\n",
    "\n",
    "def p_error(t):\n",
    "    print(\"Syntax error at '%s'\" % t.value)\n",
    "\n",
    "parser = yacc.yacc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides some housekeeping functions, the grammar specification should look very familiar at this point.\n",
    "It is essentially the same specification we have been working with.  There is one difference: terminal\n",
    "symbols are now in quotes.\n",
    "\n",
    "Please note that grammar specifications have to reside in files. Therefore, we have the magic command\n",
    "`%load` to load and the display the file.\n",
    "In order to parse an input stream in our Jupyter notebook we import the parser from the `exp0_gram.py` file\n",
    "and then run it on some input,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Token 'NEWLINE' defined, but not used\n",
      "WARNING: There is 1 unused token\n",
      "Generating LALR tables\n"
     ]
    }
   ],
   "source": [
    "from exp0_gram import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse the program `p + 1 2 ;`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser.parse(input=\"p + 1 2 ;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one would expect this was successful since the program is syntactically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{YACC}\n",
    "-->\n",
    "\n",
    "Another parser generator that is widely used is YACC (Yet Another Compiler Compiler).\n",
    "It was developed as part of the original Unix system and became the defacto standard for\n",
    "LR(1) parser generators.  Notice that Ply gives homage to this lineage in that its parser constructor\n",
    "is called `yacc()`.\n",
    "An open source version of YACC is available as Bison at \n",
    "<a href=\"http://www.gnu.org/software/bison\">www.gnu.org/software/bison</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example: Our First Language Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{language processor}\n",
    "\\index{rvalue}\n",
    "\\index{lvalue}\n",
    "\\index{variable reference}\n",
    "-->\n",
    "\n",
    "Let us put this all together and build our first language processor.  Our processor will read an Exp0 program, count the number\n",
    "of times the program references the value of a variable, and then prints the number of value references to the terminal.\n",
    "In Exp0 the only place where we can access the value of a variable is within an expression -- see the Exp0\n",
    "grammar in the previous section.\n",
    "Notice that the occurrence of a variable within an expression is very different from a variable occurrence within a `store` statement (at least when in occurs as the first argument to the `store` statement).\n",
    "When we have a variable reference within an expression then we are interested in retrieving the value that is stored in the variable.\n",
    "On the other hand, if we have a variable reference as the first argument to the `store` statement then we are interested in\n",
    "changing the value that is stored in the variable.\n",
    "In the traditional terminology of language processors we often refer to variable references within expressions as *rvalues* and  \n",
    "variable references that allow us to change the value of the variable as *lvalues*.\n",
    "This terminology is based on the fact that variable references to the right of an assignment operator access the value of a variable, the rvalue, and\n",
    "variable reference to the left of an assignment operator modify the storage location of the variable, the lvalue.\n",
    "\n",
    "Given these two distinct ways of referencing variables and given that in this case we are only interested in rvalues we have to build a language processor that understands the difference between lvalues and rvalues.  \n",
    "That means our language processor has to parse the input and only count the variable occurrences within expressions and ignore the lvalues.\n",
    "A program that does simple pattern matching on the variable names will not work here.\n",
    "\n",
    "Here is the grammar specification,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load code/chap02/exp0_count.py\n",
    "from ply import yacc\n",
    "from exp0_stuff import tokens, lexer\n",
    "\n",
    "count = 0\n",
    "\n",
    "def p_prog(_):\n",
    "    '''\n",
    "    prog : stmt_list\n",
    "    '''\n",
    "    print(\"count = {}\".format(count))\n",
    "    \n",
    "def p_stmt_list(_):\n",
    "    '''\n",
    "    stmt_list : stmt stmt_list\n",
    "              | empty\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def p_stmt(_):\n",
    "    '''\n",
    "    stmt : 'p' exp ';'\n",
    "         | 's' var exp ';'\n",
    "    '''\n",
    "    pass\n",
    "         \n",
    "def p_exp(_):\n",
    "    '''\n",
    "    exp : '+' exp exp\n",
    "        | '-' exp exp\n",
    "        | '(' exp ')'\n",
    "        | num\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def p_exp_var(_):\n",
    "    '''\n",
    "    exp : var\n",
    "    '''\n",
    "    global count\n",
    "    count += 1\n",
    "\n",
    "def p_var(_):\n",
    "    '''\n",
    "    var : 'x' \n",
    "        | 'y' \n",
    "        | 'z'\n",
    "    '''\n",
    "    pass\n",
    "        \n",
    "def p_num(_):\n",
    "    '''\n",
    "    num : '0' \n",
    "        | '1' \n",
    "        | '2' \n",
    "        | '3' \n",
    "        | '4' \n",
    "        | '5' \n",
    "        | '6' \n",
    "        | '7' \n",
    "        | '8' \n",
    "        | '9'\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def p_empty(p):\n",
    "    'empty :'\n",
    "    pass\n",
    "\n",
    "def p_error(t):\n",
    "    print(\"Syntax error at '%s'\" % t.value)\n",
    "\n",
    "def init_count():\n",
    "    global count\n",
    "    count = 0\n",
    "\n",
    "parser = yacc.yacc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we broke the grammar into smaller chunks so that we can attach code (called *embedded actions*) to the individual rules.  In particular, the first rule that defines the non-terminal `prog`.  A soon as this is parsed we print\n",
    "out the count which is a count of the number of times we saw a variable in an expression.\n",
    "That brings us to the other embedded action for the rule `exp : var`.  Everytime we use this rule to parse a \n",
    "variable in an expression we use the embedded action to increment our count.  There is no problem with confusing\n",
    "lvalues and rvalues for variables.\n",
    "\n",
    "Let's run this simple language processor on some sample input,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Token 'NEWLINE' defined, but not used\n",
      "WARNING: There is 1 unused token\n",
      "Generating LALR tables\n"
     ]
    }
   ],
   "source": [
    "from exp0_count import parser, init_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 1\n"
     ]
    }
   ],
   "source": [
    "init_count()\n",
    "parser.parse(input=\"s x + y 1 ;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our processor did the right thing.  Even though there are two variables in the source program our processor only\n",
    "counted the occurrence in the expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something a bit more ambitious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 4\n"
     ]
    }
   ],
   "source": [
    "init_count()\n",
    "parser.parse(input=\n",
    "'''\n",
    "s x 1;\n",
    "p x;\n",
    "s y 2;\n",
    "p y;\n",
    "p (+ x y);\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our processor correctly identified four rvalue variable references.\n",
    "Documentation for Ply can be found <a href=\"http://www.dabeaz.com/ply/ply.html\">here</a> and a slightly more\n",
    "elaborate example in the form of a calculator language can be found <a href=\"http://www.dabeaz.com/ply/example.html\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{lexical structure}\n",
    "\\index{lexical rule}\n",
    "\\index{token}\n",
    "-->\n",
    "\n",
    "The language generated by our Exp0 grammar only allows single character words, that is, keywords,  variable names, and numbers are all restricted to a single character.\n",
    "However, real programming languages allow longer words as part of their syntax.\n",
    "This introduces the notion of lexical structure of a programming language.\n",
    "Furthermore, the lexical structure of a programming language is \n",
    "specified with rules similar to grammar rules.  These rules are called *regular expressions*.\n",
    "\n",
    "The lexical specification of a programming language defines how individual symbols are combined to form words.\n",
    "These words are then grouped into tokens which are the entities that parser generators work with when converting a grammar\n",
    "specification into parsers.\n",
    "The tokens defined in lexical rules act similarly to non-terminals in grammar rules in that they summarize\n",
    "the lexical structure of a group of related words.  Parser generators use the lexical rules to generate *lexers* that group symbols into tokens.  Figure 8 shows how the lexical structure of a programming language is related to the phrase structure of that language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center>\n",
    "<img src=\"figures/chap02/8/figure/Slide1.jpg\" alt=\"\">\n",
    "Fig 8. The specification of the syntax of a programming language.\n",
    "</center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the lexical structure of programming languages, parsing now proceeds conceptually as a two stage process.  First the input stream is tokenized by the lexer and the tokens are sent \n",
    "to the parser as a token stream in order to recognize the phrase structure of the program.\n",
    "\n",
    "In order to get a feel of what a lexical specification looks like we will work through a simple example from the\n",
    "<a href=\"http://www.dabeaz.com/ply/ply.html#ply_nn4\">Ply documentation</a>.  The following is a Ply specification\n",
    "of a lexer for a simple arithmetic language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load code/chap02/calc_lex.py\n",
    "import ply.lex as lex\n",
    "\n",
    "# List of token names.   This is always required\n",
    "tokens = (\n",
    "   'NUMBER',\n",
    "   'PLUS',\n",
    "   'MINUS',\n",
    "   'TIMES',\n",
    "   'DIVIDE',\n",
    "   'LPAREN',\n",
    "   'RPAREN',\n",
    ")\n",
    "\n",
    "# Regular expression rules for simple tokens\n",
    "t_PLUS    = r'\\+'\n",
    "t_MINUS   = r'-'\n",
    "t_TIMES   = r'\\*'\n",
    "t_DIVIDE  = r'/'\n",
    "t_LPAREN  = r'\\('\n",
    "t_RPAREN  = r'\\)'\n",
    "\n",
    "# A regular expression rule with some action code\n",
    "def t_NUMBER(t):\n",
    "    r'[0-9]+'\n",
    "    t.value = int(t.value)    \n",
    "    return t\n",
    "\n",
    "# Define a rule so we can track line numbers\n",
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)\n",
    "\n",
    "# A string containing ignored characters (spaces and tabs)\n",
    "t_ignore  = ' \\t'\n",
    "\n",
    "# Error handling rule\n",
    "def t_error(t):\n",
    "    print(\"Illegal character '%s'\" % t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "\n",
    "# Build the lexer\n",
    "lexer = lex.lex()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the specification by listing all the token names for our language.  Here we see tokens for numbers as well as operators of our language.  Next we define the lexical structure of each of the tokens.\n",
    "These rules are very similar to grammar rules in the sense that the token names act as non-terminals and the right\n",
    "side of the rules define the structure attached to these names.  In this case the structure for these tokens consists\n",
    "of single characters defining the operators. We use Python raw strings here so we can use single escape backslashes.\n",
    "\n",
    "Next we define the lexical structure of tokens that need additional embedded actions.\n",
    "Take for example the token `t_NUMBER`.  The structure of this token is defined by the regular expression `[0-9]+`.\n",
    "This regular expression makes use of the reserved class `[0-9]` which represents all digits.  The plus sign\n",
    "specifies that the token `t_NUMBER` consists of 'one or more' digits.\n",
    "We then convert the string representing the value into an actual integer value and attach that value \n",
    "to the token structure.\n",
    "\n",
    "Finally, we define that lexical structure of three built-in tokens: `t_newline`, `t_ignore`, and `t_error`.\n",
    "Each lexer specification has to provide these. On the last line we construct the lexer object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test drive this lexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from calc_lex import lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = '22 * 3 + 45'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexer.input(input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(NUMBER,22,1,0)\n",
      "LexToken(TIMES,'*',1,3)\n",
      "LexToken(NUMBER,3,1,5)\n",
      "LexToken(PLUS,'+',1,7)\n",
      "LexToken(NUMBER,45,1,9)\n"
     ]
    }
   ],
   "source": [
    "for tok in lexer:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the token stream consists of five tokens.\n",
    "As expected the token `TIMES` has the value '`*`' and the token `PLUS` has the value '`+`'.\n",
    "More interesting are the `NUMBER` tokens.  We see that the first `NUMBER` token has the value 22, the second\n",
    "the value 3, and the third the value 45.  Which is consistent with the input stream.\n",
    "\n",
    "This illustrates that the same token class can take on different values depending on the input stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example: The Exp1 Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extend Exp0 to include multi-character keywords and variable names as well as numbers that contain more than a single digit.\n",
    "The following is the grammar specification of Exp1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load code/chap02/exp1_gram.py\n",
    "from ply import yacc\n",
    "from exp1_lex import tokens, lexer\n",
    "\n",
    "def p_grammar(_):\n",
    "    \"\"\"\n",
    "    prog : stmt_list\n",
    "    \n",
    "    stmt_list : stmt stmt_list\n",
    "              | empty\n",
    "              \n",
    "    stmt : PRINT exp ';'\n",
    "         | STORE var exp ';'\n",
    "         \n",
    "    exp : '+' exp exp\n",
    "        | '-' exp exp\n",
    "        | '(' exp ')'\n",
    "        | var\n",
    "        | num\n",
    "        \n",
    "    var : NAME\n",
    "        \n",
    "    num : NUMBER\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def p_empty(p):\n",
    "    'empty :'\n",
    "    pass\n",
    "\n",
    "def p_error(t):\n",
    "    print(\"Syntax error at '%s'\" % t.value)\n",
    "\n",
    "parser = yacc.yacc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structurally the grammar looks very similar to the Exp0 grammar but we have inserted tokens for the `store` and `print` keywords as well as for variable names and numbers.\n",
    "\n",
    "Let's take a look at the lexer for this language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load code/chap02/exp1_lex.py\n",
    "# Lexer for Exp1\n",
    "\n",
    "from ply import lex\n",
    "\n",
    "reserved = {\n",
    "    'store' : 'STORE',\n",
    "    'print' : 'PRINT'\n",
    "}\n",
    "\n",
    "literals = [';','+','-','(',')']\n",
    "\n",
    "tokens = ['NAME','NUMBER','NEWLINE'] + list(reserved.values())\n",
    "\n",
    "t_ignore = ' \\t'\n",
    "\n",
    "def t_NAME(t):\n",
    "    r'[a-zA-Z_][a-zA-Z_0-9]*'\n",
    "    t.type = reserved.get(t.value,'NAME')    # Check for reserved words\n",
    "    return t\n",
    "\n",
    "def t_NUMBER(t):\n",
    "    r'[0-9]+'\n",
    "    t.value = int(t.value)\n",
    "    return t\n",
    "\n",
    "def t_NEWLINE(t):\n",
    "    r'\\n'\n",
    "    t.lexer.lineno += 1\n",
    "\n",
    "def t_error(t):\n",
    "    print(\"Illegal character %s\" % t.value[0])\n",
    "    t.lexer.skip(1)\n",
    "\n",
    "# build the lexer\n",
    "lexer = lex.lex()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should not be any surprises here.  The most complicated regular expression is the lexical definition\n",
    "of a name. The regular expression,\n",
    "```\n",
    "[a-zA-Z_][a-zA-Z_0-9]*\n",
    "```\n",
    "states that names start with either a lower case character, upper case character, or underscore.  This is followed by zero or more (`*`) lower or upper case characters or numbers or an underscore.\n",
    "\n",
    "Let's test this language specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from exp1_gram import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_stream = \"store x1 10 ; print + x1 1 ;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser.parse(input=input_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!  Notice that Exp1 is starting to look like a real programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{structural token}\n",
    "-->\n",
    "\n",
    "A closer look at our lexical specification reveals that we have two kinds of tokens: One kind of token consists only of a single \n",
    "word\n",
    "(*e.g.*, consider the token `PRINT`, the only word belonging to this token is the word `print`).\n",
    "We call tokens that only consist of a single word *structural tokens* and they typically represent the keywords and operators of the programming language being specified.\n",
    "The other kind of\n",
    "token has a possibly infinite number of words (*e.g.*, consider the token `NUMBER`).\n",
    "These tokens usually represent values or other entities that are not rigidly define in the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\\index{regular expression}\n",
    "-->\n",
    "\n",
    "In the previous section we informally introduced regular expressions as a way to specify the structure of words belonging\n",
    "to a token.  It is time to formalize this.  Here is definition of most common regular expressions,\n",
    "\n",
    "1. Each letter `A` through `Z` and `a` through `z` is a regular expression.\n",
    "\n",
    "2. Each number `0` through `9` is a regular expression.\n",
    "\n",
    "3. Each printable character `(`, `)`, `-`, `+`, *etc.* is a regular expression.\n",
    "\n",
    "4. If *A* and *B* are regular expressions then *AB* is also a regular expression and represents the concatenation\n",
    " of the two regular expressions.\n",
    " \n",
    "5. If *A* is a regular expression then `(`*A*`)` is also a regular expression.  Parentheses allow us to group regular expressions.  If we wanted to have parentheses be part of the regular expression itself we would have to 'escape' them.  That is, `(`*A*`)` is different from `\\(`*A*`\\)`.\n",
    "\n",
    "6. If *A* and *B* are regular expressions then *A*`|`*B* is also a regular expression and represents the choice between\n",
    " regular expressions: *A* or *B*.\n",
    "\n",
    "7. If *A* is a regular expression then *A*`?` is also a regular expression and specifies the regular expression *A* as optional.\n",
    "\n",
    "8. If *A* is a regular expression then *A*`*` is also a regular expression and specifies that the regular expression *A* can appear zero or more times.\n",
    "\n",
    "9. If *A* is a regular expression then *A*`+` is also a regular expression and specifies that the regular expression *A* can appear one or more times. \n",
    "\n",
    "In addition to these definitions of regular expressions many tools provide extensions that make\n",
    "the definition of tokens easier.  The Python regular expression syntax also defines the following\n",
    "regular expressions:\n",
    "\n",
    "1. The regular expression `[A-Z]` represents a single character between `A` and `Z`.  Similarly for `[a-z]` and `[0-9]`.\n",
    "\n",
    "2. The dot operator is a regular expression that represents any single printable character.\n",
    "\n",
    "3. The special characters `\\n`, `\\t`, and `\\r` are also regular expressions\n",
    "representing the newline character, the tab character, and the carriage return character, respectively.\n",
    "\n",
    "4. The `^` operator computes the complement of a set.  For example, if we have the regular expression `[abc]` matching either `a`, or `b`, or `c`, then the complement `[^abc]` will match any character other than `a`, `b`, or `c`.  This is useful in\n",
    "conjunction with character classes.  For example, the regular expression \n",
    "`[A-Z][^A-Z]`\n",
    "specifies a word structure that starts with a capital letter followed by a single character that is not a capital letter.\n",
    "\n",
    "More details on regular expressions can be found in \n",
    "<a href=\"https://docs.python.org/3/library/re.html\">Python's regular expression documentation</a>.\n",
    "\n",
    "Just to exercise this machinery a bit here is a regular expression for integer values,\n",
    "```\n",
    "0 | -?[1-9][0-9]*\n",
    "```\n",
    "First notice that this expression consists of two regular expressions, namely `0` and  `-?[1-9][0-9]*`,\n",
    "joined together by the `|` operator denoting a choice.  \n",
    "The first regular expression is just the character `0` denoting the\n",
    "fact that one choice is just the digit `0`.  The second regular expression is the concatenation of three different \n",
    "regular expressions: `-?`, `[1-9]`, and `[0-9]*`.  The first one denotes an optional minus sign, the second\n",
    "denotes a single digit between 1 and 9, and the last one denotes zero or more digits between 0 and 9.\n",
    "The concatenation of the three regular expressions represents possibly negative numbers that start with a digit between 1 and 9 followed \n",
    "by zero or more digits between 0 and 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started this chapter with a discussion of grammars and defining languages using grammars.  Derivations within grammars allow\n",
    "us to show that a particular sentence is a valid sentence and belongs to the language of the grammar.  \n",
    "Once we have a derivation it is straightforward to construct a parse tree and we can view parse trees as a visual representation of\n",
    "derivations.\n",
    "\n",
    "The construction of derivations can be mechanized using a parsing algorithm.  We classify parsing algorithm according to how they\n",
    "construct the associated derivation or parse tree: Top-down -- starting at the grammar start symbol.  Bottom-up -- starting at the leaves\n",
    "of the parse tree.  Top-down parsing algorithms are also called LL(k) algorithms in that they read the input from the (L)eft, construct\n",
    "the (L)eft-most derivation, and use (k) lookahead symbols.  Bottom-up parsing algorithms are also called LR(k) algorithms because\n",
    "they read the input from the (L)eft, build the (R)ight-most derivation, and use (k) lookahead symbols.  Here we studied the LL(1) and LR(0)\n",
    "parsing algorithms in more detail.\n",
    "We also introduced the parser generator Ply we will be using for the remainder of this book.  Ply generates LR(1)\n",
    "parsers.\n",
    "\n",
    "Practical parsing of programming languages is done as a two step process: First we perform the lexical analysis, that is, we analyze the structure\n",
    "of the language words.  Then we perform the syntactic or phrase structure analysis of the program.\n",
    "The phrase or syntactical structure of a programming language is given using grammar rules while the lexical structure is specified using \n",
    "regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographic Notes\n",
    "\n",
    "A number of books introduce formal language theory, grammars, and derivations within grammars in addition to finite state machines.\n",
    "Perhaps the most accessible books are\n",
    "by Webber (Webber, 2008) and Sipser (Sipser, 2012).\n",
    "Most books on compiler construction discuss parsing theory in detail and introduce efficient algorithms for lexical and syntax analysis, *e.g.*, (Aho, 1986) and (Cooper & Torcson, 2011).\n",
    "Donald Knuth's seminal paper (Knuth, 1965) introduced LR parsing in 1965 and made parsing practical and efficient.\n",
    "Grammars with respect to operator associativity and precedence are discussed in (Webber, 2010).\n",
    "\n",
    "Webber, A. B. (2008). *Formal language: A practical introduction*. Franklin, Beedle & Associates Inc.\n",
    "\n",
    "Sipser, M. (2012). *Introduction to the Theory of Computation*. Cengage Learning.\n",
    "\n",
    "Aho, A. V., Sethi, R., & Ullman, J. D. (1986). *Compilers, Principles, Techniques*. Boston: Addison-Wesley.\n",
    "\n",
    "Cooper, K., & Torczon, L. (2011). *Engineering a compiler*. Elsevier.\n",
    "\n",
    "Knuth, D. E. (1965). *On the translation of languages from left to right*. Information and control, 8(6), 607-639.\n",
    "\n",
    "Webber, A. B. (2010). *Modern programming languages: A practical introduction.* Franklin, Beedle & Associates Inc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Define a small LL(1) language using a grammar.  Write the grammar in tuple notation as we did\n",
    "in the section on lookahead sets.  Then use the lookahead set algorithm to compute the grammar extended\n",
    "with the lookahead sets.\n",
    "\n",
    "2. Define a small left-recursive language.  Write its grammar in tuple notation as we did in the section on lookahead sets.  Convince yourself that the lookahead set algorithm does not work\n",
    "on this grammar. Is it LR(0)?\n",
    "\n",
    "3. Use the Exp0 grammar in tuple notation and the top-down parsing algorithm and show that you can parse\n",
    "some non-trivial Exp0 programs.\n",
    "\n",
    "4. LL(0) parsers are parsers that read input from left to right, construct the left-most derivation but use no lookahead symbols.\n",
    "What does that mean for the kind of grammars such a parser could use for parsing?  \n",
    "\n",
    "5. Rewrite the lookahead set algorithm in such a way that it detects left-recursion, issues an error message, and terminates. The extension should be general enough so that it also detects left-recursion in mutually recursive rules.\n",
    "\n",
    "6. We have shown that if-then-else statements create shift-reduce conflicts in LR(0) parsers.  Show that right-recursive rules such as\n",
    "  ```\n",
    "  list : item\n",
    "        | item list\n",
    "       \n",
    "  item : e\n",
    "  ```\n",
    "also create a shift-reduce conflict in our LR(0) parser.\n",
    "Compute a trace similar to Figure 6 for the input stream `e e e \\eof`.\n",
    "\n",
    "7. Design a lexical rule that defines a token for the binary representation of positive integer numbers.\n",
    "\n",
    "8. Design a lexical rule defines a token for the floating point numbers (don't include the scientific notation).\n",
    "\n",
    "9. Write the following grammar as a Ply specification (including both lexical and syntax specifications), and parse both correct and incorrect input streams.\n",
    "  ```\n",
    "  list : [ element_list ]\n",
    "  \n",
    "  element_list : element_list , element\n",
    "                | element\n",
    "  \t\n",
    "  element : a | b | c\n",
    "  ```\n",
    "\n",
    "10. Write a grammar for the arithmetic expressions in your favorite programming language.\n",
    "\n",
    "11. (project) Extend the top-down parsing algorithm so that it constructs and returns a parse tree.\n",
    "\n",
    "12. (project) Extend the bottom-up parsing algorithm so that it constructs and returns a parse tree.\n",
    " \n",
    "13. (project)  Extend the bottom-up parsing algorithms so that it flags the all rules that either create an LR(0) reduce-reduce conflict or a shift-reduce conflict.\n",
    " \n",
    "14. (project) Reduce-reduce conflicts in bottom-up parsers can be resolved by guessing and backtracking: guess one alternative and try to parse the sentence; if this is not possible backtrack to the decision point and try another alternative. Extend the bottom-up parsing algorithm with backtracking on reduce-reduce conflicts and implement a parser using your extended algorithm that reads a grammar  and an input stream and returns `True` if the program in the input stream is a valid sentence in the language of the grammar otherwise `False`.\n",
    "\n",
    "15. (project) Build a recursive descent parser for the Exp0 programming language.  Show that it works by parsing some sentences that are valid in Exp0.  Show that it rejects invalid sentences with syntax errors.\n",
    "\n",
    "16. (project) Modify your recursive descent parser from Exercise 15 to construct parse trees for input programs.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
